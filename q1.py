# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uuRqgHTtkXBlfiSq76zqWaR6hq57pw4V
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras import Model, Input
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import os
import zipfile
import matplotlib.pyplot as plt

def build_model(filters, kernel_dims, activation_function):
    model = Sequential()
    # First convolutional block
    model.add(Conv2D(filters[0], kernel_dims, activation=activation_function, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # Second convolutional block
    model.add(Conv2D(filters[1], kernel_dims, activation=activation_function))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # Third convolutional block
    model.add(Conv2D(filters[1], kernel_dims, activation=activation_function))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # Fourth convolutional block
    model.add(Conv2D(filters[1], kernel_dims, activation=activation_function))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # Fully connected layers
    model.add(Flatten())
    model.add(Dense(1024, activation=activation_function))
    model.add(Dense(3, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Unzipping dataset if not already done
dataset_zip_path = "shoes.zip"
if not os.path.isdir("shoes"):
    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:
        zip_ref.extractall("shoes")


train_data_dir = "shoes/train"
test_data_dir = "shoes/test"

# Image dimensions
IMG_HEIGHT, IMG_WIDTH = 224, 224


train_data_generator = ImageDataGenerator(
    rescale=1.0 / 255.0,
    rotation_range=25,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.25,
    zoom_range=0.25,
    horizontal_flip=True,
    brightness_range=(0.8, 1.2)
)


test_data_generator = ImageDataGenerator(rescale=1.0 / 255.0)


training_data = train_data_generator.flow_from_directory(
    train_data_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=64,
    class_mode='categorical'
)


testing_data = test_data_generator.flow_from_directory(
    test_data_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=64,
    class_mode='categorical'
)

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-6)

# Training
history = model.fit(training_data, epochs=40, validation_data=testing_data, callbacks=[early_stopping, reduce_lr])

# Evaluation
test_loss, test_accuracy = model.evaluate(testing_data)
print(f"Test accuracy: {test_accuracy * 100:.2f}%")

model.save("shoe_classifier_model.keras")

print(len(history.history['accuracy']))
print(len(history.history['val_accuracy']))

# Plot Accuracy
plt.figure(figsize=(10, 6))  # Adjusted figure size for a wider display
plt.plot(history.history['accuracy'][:100], label='Train Accuracy', linestyle='-', color='blue')
plt.plot(history.history['val_accuracy'][:100], label='Validation Accuracy', linestyle='--', color='orange')
plt.xlabel('Epoch', fontsize=12)  # Added font size for labels
plt.ylabel('Accuracy', fontsize=12)
plt.title('Accuracy Over Epochs', fontsize=14)  # Slightly changed title wording
plt.legend(loc='best')  # Adjusted legend to pick the optimal location
plt.grid(alpha=0.3)  # Added a light grid for better readability
plt.show()

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-6)

# Training
history = model.fit(training_data, epochs=100, validation_data=testing_data, callbacks=[early_stopping, reduce_lr])

# Evaluation
test_loss, test_accuracy = model.evaluate(testing_data)
print(f"Test accuracy: {test_accuracy * 100:.2f}%")

model.save("shoe_classifier_model.keras")

# Plot Accuracy
plt.figure(figsize=(10, 6))  # Adjusted figure size for a wider display
plt.plot(history.history['accuracy'], label='Train Accuracy', linestyle='-', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--', color='orange')
plt.xlabel('Epoch', fontsize=12)  # Added font size for labels
plt.ylabel('Accuracy', fontsize=12)
plt.title('Accuracy Over Epochs', fontsize=14)  # Slightly changed title wording
plt.legend(loc='best')  # Adjusted legend to pick the optimal location
plt.grid(alpha=0.3)  # Added a light grid for better readability
plt.show()

from tensorflow.keras import backend as K
K.clear_session()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adamax
from tensorflow.keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K

# Define optimizers to test
optimizers = [Adam, SGD, RMSprop, Adagrad, Adamax]

# Dictionary to store histories
history_dict = {}
# Fixed hyperparameters with reduced complexity
filters = [16, 32]  # Reduced filters
dense_units = 256    # Reduced dense units
IMG_HEIGHT, IMG_WIDTH = 128, 128
learning_rate = 0.001
epochs = 15
batch_size = 8  # Smaller batch size

# Build model function with reduced parameters for memory optimization
def build_model_with_optimizer(optimizer, learning_rate):
    model = Sequential()
    model.add(Conv2D(filters[0], (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(filters[1], (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    # Using GlobalAveragePooling instead of Flatten
    model.add(GlobalAveragePooling2D())

    # Add fully connected layer with dropout
    model.add(Dense(dense_units, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(3, activation='softmax'))

    # Compile the model
    opt = optimizer(learning_rate=learning_rate)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Train and evaluate the model for each optimizer
for optimizer in optimizers:
    print(f"\nTraining with Optimizer: {optimizer.__name__}")
    model = build_model_with_optimizer(optimizer, learning_rate)

    # Callback to adjust learning rate dynamically
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_lr=1e-6)

    # Training the model
    history = model.fit(training_data, epochs=epochs, validation_data=testing_data, batch_size=batch_size, callbacks=[reduce_lr])

    # Save training history
    history_dict[optimizer.__name__] = history.history

    # Free GPU memory after training each model
    K.clear_session()

# Plotting accuracy for each optimizer
plt.figure(figsize=(12, 8))
for opt_name, hist in history_dict.items():
    plt.plot(hist['accuracy'], label=f'Train Acc ({opt_name})')
    plt.plot(hist['val_accuracy'], label=f'Val Acc ({opt_name})', linestyle='--')

plt.title("Training and Validation Accuracy Across Optimizers", fontsize=16)
plt.xlabel("Epoch", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.legend(loc='best', fontsize=10)
plt.grid(True)
plt.tight_layout()
plt.show()

